{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pydub import AudioSegment\n",
    "from pyannote.audio import Pipeline, Audio as PyannoteAudio\n",
    "import whisper\n",
    "from pyannote.core import Segment\n",
    "import gradio as gr\n",
    "from turkish_lm_tuner import TextPredictor\n",
    "import torchaudio \n",
    "import torch\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "\n",
    "# Function to save audio array to a temporary WAV file\n",
    "def save_audio_to_wav(audio, sample_rate):\n",
    "    audio_file_path = \"temp_audio.wav\"\n",
    "    # Write the audio data to a WAV file\n",
    "    sf.write(audio_file_path, audio, sample_rate)\n",
    "    return audio_file_path\n",
    "\n",
    "# Function to convert to WAV format if necessary\n",
    "def convert_to_wav(audio_file_path):\n",
    "    # Split the file path into the file name and extension\n",
    "    file_name, file_extension = os.path.splitext(audio_file_path)\n",
    "    # Check if the file is not already in WAV format\n",
    "    if file_extension.lower() != \".wav\":\n",
    "        # Load the audio file using pydub\n",
    "        audio = AudioSegment.from_file(audio_file_path)\n",
    "        # Define the new file path with WAV extension\n",
    "        wav_file_path = f\"{file_name}.wav\"\n",
    "        # Export the audio file as WAV\n",
    "        audio.export(wav_file_path, format=\"wav\")\n",
    "        return wav_file_path\n",
    "    return audio_file_path\n",
    "\n",
    "# Function to convert stereo to mono\n",
    "def stereo_to_mono(waveform):\n",
    "    # Check if the audio is stereo (2 channels)\n",
    "    if waveform.ndim == 2 and waveform.shape[0] == 2:\n",
    "        # Average the two channels to create a mono signal\n",
    "        waveform = np.mean(waveform, axis=0)\n",
    "    return waveform\n",
    "\n",
    "# Load sentiment and topic predictors\n",
    "# Initialize the sentiment predictor with the specified model\n",
    "sentiment_predictor = TextPredictor(model_name='boun-tabi-LMG/turna_classification_17bintweet_sentiment', task='sentiment')\n",
    "# Initialize the topic predictor with the specified model\n",
    "topic_predictor = TextPredictor(model_name='boun-tabi-LMG/turna_classification_ttc4900', task='categorization')\n",
    "\n",
    "# Function to analyze text and return sentiment and topic\n",
    "def analyze_text(text):\n",
    "    # Predict sentiment of the text\n",
    "    sentiment = sentiment_predictor.predict(text)\n",
    "    # Predict topic of the text\n",
    "    topic = topic_predictor.predict(text)\n",
    "    return sentiment, topic\n",
    "\n",
    "# Function to process audio and return transcriptions\n",
    "def process_audio(audio_file_path):\n",
    "    # Ensure the audio file is in WAV format\n",
    "    audio_file_path = convert_to_wav(audio_file_path)\n",
    "\n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(audio_file_path):\n",
    "        raise RuntimeError(f\"File does not exist: {audio_file_path}\")\n",
    "\n",
    "    # Load pyannote.audio speaker diarization pipeline\n",
    "    speaker_diarization = Pipeline.from_pretrained(\"pyannote/speaker-diarization-3.1\", use_auth_token=\"YOUR_AUTH_TOKEN\")\n",
    "    # Perform speaker diarization on the audio file to determine who speaks when\n",
    "    who_speaks_when = speaker_diarization(audio_file_path, num_speakers=3)\n",
    "\n",
    "    # Load Whisper model for transcription\n",
    "    model = whisper.load_model(\"large-v3\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Initialize lists to store transcriptions and segments for each speaker\n",
    "    transcriptions = []\n",
    "    speaker_segments = {}\n",
    "    # Initialize pyannote Audio object\n",
    "    audio = PyannoteAudio(sample_rate=16000, mono=True)\n",
    "\n",
    "    # Iterate over each segment with speaker labels\n",
    "    for segment, _, speaker in who_speaks_when.itertracks(yield_label=True):\n",
    "        # Extract the waveform for the current segment\n",
    "        waveform, sample_rate = audio.crop(audio_file_path, segment)\n",
    "        waveform = waveform.squeeze().numpy()  # Convert to numpy array\n",
    "        waveform = stereo_to_mono(waveform)  # Convert to mono\n",
    "\n",
    "        # Transcribe the processed waveform using Whisper\n",
    "        text = model.transcribe(waveform, language=\"tr\")[\"text\"]\n",
    "        # Add the transcription to the list\n",
    "        transcriptions.append(f\"{segment.start:06.1f}s - {segment.end:06.1f}s - {speaker}: {text}\")\n",
    "        # Append the text to the corresponding speaker's combined text\n",
    "        if speaker not in speaker_segments:\n",
    "            speaker_segments[speaker] = \"\"\n",
    "        speaker_segments[speaker] += text + \" \"\n",
    "\n",
    "    # Analyze sentiment and topic for each speaker's combined text\n",
    "    for speaker, combined_text in speaker_segments.items():\n",
    "        sentiment, topic = analyze_text(combined_text)\n",
    "        transcriptions.append(f\"Speaker {speaker} sentiment: {sentiment[0]}, topic: {topic[0]}\")\n",
    "\n",
    "    return \"\\n\".join(transcriptions)\n",
    "\n",
    "# Function to handle the audio input and return the transcription result\n",
    "def transcribe(audio):\n",
    "    if audio is None:\n",
    "        return \"No audio input received.\"\n",
    "\n",
    "    sample_rate, audio_data = audio\n",
    "    # Convert audio data to float32 format\n",
    "    audio_data = audio_data.astype(np.float32)\n",
    "    # Normalize the audio data\n",
    "    audio_data /= np.max(np.abs(audio_data))\n",
    "\n",
    "    # Save the audio data to a WAV file\n",
    "    audio_file_path = save_audio_to_wav(audio_data, sample_rate)\n",
    "\n",
    "    transcription = process_audio(audio_file_path)  \n",
    "\n",
    "    return transcription\n",
    "\n",
    "# Create the Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn=transcribe,  # Function to call when the audio input is provided\n",
    "    inputs=gr.Audio(sources=[\"microphone\", \"upload\"], type=\"numpy\"), \n",
    "    outputs=gr.Textbox(),  \n",
    "    title=\"Speaker Diarization, Transcription, Sentiment, and Topic Analysis\"\n",
    ")\n",
    "\n",
    "# Launch the Gradio interface\n",
    "iface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
