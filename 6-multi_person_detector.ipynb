{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pydub import AudioSegment\n",
    "from pyannote.audio import Pipeline, Audio\n",
    "import whisper\n",
    "from pyannote.core import Segment\n",
    "from pyannote.database import get_protocol, FileFinder\n",
    "from pyannote.database.util import load_rttm\n",
    "\n",
    "# Function to convert audio to WAV format if not already in WAV\n",
    "def convert_to_wav(audio_file_path):\n",
    "    file_name, file_extension = os.path.splitext(audio_file_path)\n",
    "    if file_extension.lower() != \".wav\":\n",
    "        audio = AudioSegment.from_file(audio_file_path)\n",
    "        wav_file_path = f\"{file_name}.wav\"\n",
    "        audio.export(wav_file_path, format=\"wav\")\n",
    "        return wav_file_path\n",
    "    return audio_file_path\n",
    "\n",
    "# same directory as this script\n",
    "audio_file_path = \"deneme.wav\"\n",
    "audio_file_path = convert_to_wav(audio_file_path)\n",
    "\n",
    "speaker_diarization = Pipeline.from_pretrained(\"pyannote/speaker-diarization-3.1\", use_auth_token=\"AUTH_TOKEN\")\n",
    "\n",
    "# Apply speaker diarization and save to RTTM format\n",
    "# We are saving this in the rrtm file because if the code fails, we can still have the output of the speaker diarization\n",
    "# and it is easier to debug the model and work on it\n",
    "who_speaks_when = speaker_diarization(audio_file_path, num_speakers=2)  # Assuming 2 speakers (can be more or less with max and min also specified)\n",
    "with open(\"audio2.rttm\", \"w\") as rttm:\n",
    "    who_speaks_when.write_rttm(rttm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Whisper model\n",
    "model = whisper.load_model(\"large-v3\")\n",
    "\n",
    "# Save transcriptions to a file\n",
    "output_file = \"transcriptions_with_speakers2.txt\"\n",
    "\n",
    "# Load the RTTM file to get diarization segments\n",
    "diarization = load_rttm(\"audio2.rttm\")\n",
    "\n",
    "# Process each speaker segment and save transcriptions\n",
    "audio = Audio(sample_rate=16000, mono=True)\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f_out:\n",
    "    for file_id, turns in diarization.items():\n",
    "        for segment, track, label in turns.itertracks(yield_label=True):\n",
    "            waveform, sample_rate = audio.crop(audio_file_path, segment)  # waveform is a torch tensor\n",
    "            # Convert waveform tensor to a numpy array and then to a list\n",
    "            waveform_np = waveform.squeeze().numpy()\n",
    "            result = model.transcribe(waveform_np, language=\"tr\")\n",
    "            text = result[\"text\"]\n",
    "            f_out.write(f\"{segment.start:06.1f}s - {segment.end:06.1f}s - {label}: {text}\\n\")\n",
    "            print(f\"{segment.start:06.1f}s - {segment.end:06.1f}s - {label}: {text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "from pydub import AudioSegment\n",
    "from pyannote.audio import Pipeline, Audio\n",
    "from pyannote.core import Segment\n",
    "import whisper\n",
    "\n",
    "# Function to convert audio to WAV format if not already in WAV\n",
    "def convert_to_wav(audio_file_path):\n",
    "    file_name, file_extension = os.path.splitext(audio_file_path)\n",
    "    if file_extension.lower() != \".wav\":\n",
    "        audio = AudioSegment.from_file(audio_file_path)\n",
    "        wav_file_path = f\"{file_name}.wav\"\n",
    "        audio.export(wav_file_path, format=\"wav\")\n",
    "        return wav_file_path\n",
    "    return audio_file_path\n",
    "\n",
    "# Same directory as this script\n",
    "audio_file_path = \"deneme.wav\"\n",
    "audio_file_path = convert_to_wav(audio_file_path)\n",
    "\n",
    "# Load pyannote.audio speaker diarization pipeline\n",
    "speaker_diarization = Pipeline.from_pretrained(\"pyannote/speaker-diarization-3.1\", use_auth_token=\"AUTH_TOKEN\")\n",
    "\n",
    "# Apply speaker diarization\n",
    "who_speaks_when = speaker_diarization(audio_file_path, num_speakers=2, min_speakers=None, max_speakers=None)\n",
    "\n",
    "# Load OpenAI Whisper automatic speech transcription model\n",
    "model = whisper.load_model(\"large-v3\")\n",
    "\n",
    "# Create an audio object for cropping\n",
    "audio = Audio(sample_rate=16000, mono=True)\n",
    "\n",
    "# Transcribe each speaker segment in the first minute\n",
    "for segment, _, speaker in who_speaks_when.itertracks(yield_label=True):\n",
    "    waveform, sample_rate = audio.crop(audio_file_path, segment)\n",
    "    text = model.transcribe(waveform.squeeze().numpy(), language=\"tr\")[\"text\"]\n",
    "    print(f\"{segment.start:06.1f}s {segment.end:06.1f}s {speaker}: {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "from pydub import AudioSegment\n",
    "from pyannote.audio import Pipeline, Audio\n",
    "from pyannote.core import Segment\n",
    "import whisper\n",
    "\n",
    "# Function to convert audio to WAV format if not already in WAV\n",
    "def convert_to_wav(audio_file_path):\n",
    "    file_name, file_extension = os.path.splitext(audio_file_path)\n",
    "    if file_extension.lower() != \".wav\":\n",
    "        audio = AudioSegment.from_file(audio_file_path)\n",
    "        wav_file_path = f\"{file_name}.wav\"\n",
    "        audio.export(wav_file_path, format=\"wav\")\n",
    "        return wav_file_path\n",
    "    return audio_file_path\n",
    "\n",
    "# Same directory as this script\n",
    "audio_file_path = \"deneme.wav\"\n",
    "audio_file_path = convert_to_wav(audio_file_path)\n",
    "\n",
    "# Load pyannote.audio speaker diarization pipeline\n",
    "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization-3.1\", use_auth_token=\"AUTH_TOKEN\")\n",
    "import torch\n",
    "pipeline.to(torch.device(\"cuda\"))\n",
    "\n",
    "# Apply speaker diarization\n",
    "who_speaks_when = pipeline(audio_file_path, num_speakers=2, min_speakers=None, max_speakers=None)\n",
    "\n",
    "# Load OpenAI Whisper automatic speech transcription model\n",
    "model = whisper.load_model(\"large-v3\")\n",
    "\n",
    "# Create an audio object for cropping\n",
    "audio = Audio(sample_rate=16000, mono=True)\n",
    "\n",
    "# Transcribe each speaker segment in the first minute\n",
    "for segment, _, speaker in who_speaks_when.itertracks(yield_label=True):\n",
    "    waveform, sample_rate = audio.crop(audio_file_path, segment)\n",
    "    #text = model.transcribe(waveform.squeeze().numpy(), language=\"tr\")[\"text\"]\n",
    "    #print(f\"{segment.start:06.1f}s {segment.end:06.1f}s {speaker}: {text}\")\n",
    "    print(waveform.squeeze().numpy())\n",
    "    print(waveform.squeeze().numpy().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import ffmpeg\n",
    "from pydub import AudioSegment\n",
    "from pyannote.audio import Pipeline, Audio\n",
    "from pyannote.core import Segment\n",
    "import whisper\n",
    "\n",
    "# Function to convert audio to WAV format if not already in WAV\n",
    "def convert_to_wav(audio_file_path):\n",
    "    file_name, file_extension = os.path.splitext(audio_file_path)\n",
    "    if file_extension.lower() != \".wav\":\n",
    "        audio = AudioSegment.from_file(audio_file_path)\n",
    "        wav_file_path = f\"{file_name}.wav\"\n",
    "        audio.export(wav_file_path, format=\"wav\")\n",
    "        return wav_file_path\n",
    "    return audio_file_path\n",
    "\n",
    "# Function to resample audio to 16kHz\n",
    "def resample_audio(waveform, orig_sr, target_sr=16000):\n",
    "    if orig_sr != target_sr:\n",
    "        out, _ = (\n",
    "            ffmpeg.input('pipe:', format='f32le', ac=1, ar=orig_sr)\n",
    "            .output('pipe:', format='f32le', ac=1, ar=target_sr)\n",
    "            .run_async(pipe_stdin=True, pipe_stdout=True)\n",
    "            .communicate(input=waveform.tobytes())\n",
    "        )\n",
    "        waveform = np.frombuffer(out, np.float32)\n",
    "    return waveform\n",
    "\n",
    "# Function to convert stereo to mono\n",
    "def stereo_to_mono(waveform):\n",
    "    if waveform.shape[0] == 2:  # Check if the audio is stereo\n",
    "        waveform = np.mean(waveform, axis=0)  # Average the two channels\n",
    "    return waveform\n",
    "\n",
    "# Same directory as this script\n",
    "audio_file_path = \"deneme.wav\"\n",
    "audio_file_path = convert_to_wav(audio_file_path)\n",
    "\n",
    "# Load pyannote.audio speaker diarization pipeline\n",
    "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization-3.1\", use_auth_token=\"YOUR_AUTH_TOKEN\")\n",
    "import torch\n",
    "pipeline.to(torch.device(\"cuda\"))\n",
    "\n",
    "# Apply speaker diarization\n",
    "who_speaks_when = pipeline(audio_file_path, num_speakers=2)\n",
    "\n",
    "# Load OpenAI Whisper automatic speech transcription model\n",
    "model = whisper.load_model(\"large-v3\")\n",
    "\n",
    "# Create an audio object for cropping\n",
    "audio = Audio(sample_rate=16000, mono=True)\n",
    "\n",
    "# Transcribe each speaker segment\n",
    "for segment, _, speaker in who_speaks_when.itertracks(yield_label=True):\n",
    "    waveform, sample_rate = audio.crop(audio_file_path, segment)\n",
    "    waveform = waveform.squeeze().numpy()\n",
    "    waveform = stereo_to_mono(waveform)\n",
    "    waveform = resample_audio(waveform, sample_rate, target_sr=16000)\n",
    "    text = model.transcribe(waveform, language=\"tr\")[\"text\"]\n",
    "    print(f\"{segment.start:06.1f}s {segment.end:06.1f}s {speaker}: {text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import ffmpeg\n",
    "from pydub import AudioSegment\n",
    "from pyannote.audio import Pipeline, Audio\n",
    "from pyannote.core import Segment\n",
    "import whisper\n",
    "\n",
    "# Function to convert audio to WAV format if not already in WAV\n",
    "def convert_to_wav(audio_file_path):\n",
    "    file_name, file_extension = os.path.splitext(audio_file_path)\n",
    "    if file_extension.lower() != \".wav\":\n",
    "        audio = AudioSegment.from_file(audio_file_path)\n",
    "        wav_file_path = f\"{file_name}.wav\"\n",
    "        audio.export(wav_file_path, format=\"wav\")\n",
    "        return wav_file_path\n",
    "    return audio_file_path\n",
    "\n",
    "# Function to convert stereo to mono\n",
    "def stereo_to_mono(waveform):\n",
    "    if waveform.shape[0] == 2:  # Check if the audio is stereo\n",
    "        waveform = np.mean(waveform, axis=0)  # Average the two channels\n",
    "    return waveform\n",
    "\n",
    "# Same directory as this script\n",
    "audio_file_path = \"deneme.mp3\"\n",
    "audio_file_path = convert_to_wav(audio_file_path)\n",
    "\n",
    "# Load pyannote.audio speaker diarization pipeline\n",
    "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization-3.1\", use_auth_token=\"AUTH_TOKEN\")\n",
    "import torch\n",
    "#pipeline.to(torch.device(\"cuda\"))\n",
    "\n",
    "# Apply speaker diarization\n",
    "who_speaks_when = pipeline(audio_file_path, num_speakers=3)\n",
    "\n",
    "# Load OpenAI Whisper automatic speech transcription model\n",
    "model = whisper.load_model(\"large-v3\")\n",
    "\n",
    "# Create an audio object for cropping\n",
    "audio = Audio(sample_rate=16000, mono=True)\n",
    "\n",
    "# Transcribe each speaker segment\n",
    "for segment, _, speaker in who_speaks_when.itertracks(yield_label=True):\n",
    "    # Crop the audio segment\n",
    "    waveform, sample_rate = audio.crop(audio_file_path, segment)\n",
    "    waveform = waveform.squeeze().numpy()  # Convert to numpy array\n",
    "    # Convert to mono and resample to 16000 Hz\n",
    "    waveform = stereo_to_mono(waveform)    \n",
    "    # Transcribe the processed waveform\n",
    "    text = model.transcribe(waveform, language=\"tr\")[\"text\"]\n",
    "    print(f\"{segment.start:06.1f}s {segment.end:06.1f}s {speaker}: {text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "who_speaks_when"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
